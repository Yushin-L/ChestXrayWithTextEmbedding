{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28447810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/mimic/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import json\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from transformers import ViTFeatureExtractor \n",
    "import ast\n",
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2embedding(client, model, text):\n",
    "    responses = client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=model,\n",
    "        )\n",
    "    return responses.data[0].embedding\n",
    "\n",
    "openai_api_key = \"abc123\"\n",
    "openai_api_base = \"http://localhost:8002/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "models = client.models.list()\n",
    "model = models.data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484ca45",
   "metadata": {},
   "source": [
    "# Rex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b364173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_medical_data_to_dataframe(data_dict):\n",
    "    \"\"\"\n",
    "    의료 데이터 딕셔너리를 DataFrame으로 변환하면서 \n",
    "    이미지 관련 정보를 개별 행으로 확장하는 함수\n",
    "    Args:\n",
    "        data_dict: 의료 데이터가 담긴 딕셔너리\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: 확장된 데이터프레임\n",
    "    \"\"\"\n",
    "    # 이미지 관련 필드들 (리스트 형태로 되어있는 필드들)\n",
    "    image_fields = ['ImagePath', 'ImageModality', 'ImageShape', 'ImageBodyPart', 'ImageViewPosition']\n",
    "    # 이미지 개수 확인\n",
    "    n_images = len(data_dict['ImagePath'])\n",
    "    # 결과를 담을 리스트\n",
    "    rows = []\n",
    "    # 각 이미지에 대해 행을 생성\n",
    "    for i in range(n_images):\n",
    "        row = {}\n",
    "        \n",
    "        # 기본 정보들 (모든 행에 동일하게 복사)\n",
    "        for key, value in data_dict.items():\n",
    "            if key not in image_fields:\n",
    "                row[key] = value\n",
    "            else:\n",
    "                # 이미지 관련 정보는 해당 인덱스의 값을 사용\n",
    "                if isinstance(value, list) and i < len(value):\n",
    "                    row[key] = value[i]\n",
    "                else:\n",
    "                    row[key] = None\n",
    "        rows.append(row)\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "def process_multiple_medical_records(data_list):\n",
    "    \"\"\"\n",
    "    여러 의료 기록 딕셔너리를 처리하는 함수\n",
    "    \n",
    "    Args:\n",
    "        data_list: 의료 데이터 딕셔너리들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: 모든 기록을 확장한 통합 데이터프레임\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    for data_dict in data_list:\n",
    "        df = expand_medical_data_to_dataframe(data_dict)\n",
    "        all_rows.append(df)\n",
    "    # 모든 DataFrame을 하나로 합치기\n",
    "    combined_df = pd.concat(all_rows, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def standardize_view_position_direct(df, column_name='ImageViewPosition'):\n",
    "    \"\"\"\n",
    "    딕셔너리를 사용한 직접 매핑 방식\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'PA': 'PA',\n",
    "        'POSTERO_ANTERIOR': 'PA',\n",
    "        'AP': 'AP', \n",
    "        'ANTERO_POSTERIOR': 'AP',\n",
    "        'AP AXIAL': 'AP'\n",
    "    }\n",
    "    \n",
    "    df_standardized = df.copy()\n",
    "    df_standardized[column_name] = df_standardized[column_name].map(mapping).fillna(df_standardized[column_name])\n",
    "    \n",
    "    return df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2607a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/ReXGradient-160K/metadata/train_metadata.csv')\n",
    "with open('/data/ReXGradient-160K/metadata/train_metadata_view_position.json', 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "df2 = process_multiple_medical_records(list(json_data.values()))\n",
    "df2 = standardize_view_position_direct(df2)\n",
    "df2 = df2[(df2['ImageViewPosition']=='AP') | (df2['ImageViewPosition']=='PA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_rows = []\n",
    "for idx, row in tqdm(df2.iterrows()):\n",
    "    note = \"Findings: {} \\nImpression: {}\".format(row['Findings'], row['Impression'])\n",
    "    embedding = text2embedding(client, model, note)\n",
    "    embedding_rows.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee95b74",
   "metadata": {},
   "source": [
    "# Mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_view_position_direct(df, column_name='ViewPosition'):\n",
    "    \"\"\"\n",
    "    딕셔너리를 사용한 직접 매핑 방식\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'PA': 'PA',\n",
    "        'PA LLD': 'PA',\n",
    "        'PA RLD': 'PA',\n",
    "        'AP': 'AP', \n",
    "        'AP AXIAL': 'AP',\n",
    "        'AP LLD': 'AP',\n",
    "        'AP RLD': 'AP'\n",
    "    }\n",
    "    \n",
    "    df_standardized = df.copy()\n",
    "    df_standardized[column_name] = df_standardized[column_name].map(mapping).fillna(df_standardized[column_name])\n",
    "    \n",
    "    return df_standardized\n",
    "\n",
    "def load_text(path):\n",
    "    with open(path,'r') as file:\n",
    "        lines=file.readlines()\n",
    "        file_content=''.join(lines)\n",
    "    return file_content.split(\"FINAL REPORT\\n \")[1].replace('\\n ','\\n') #\n",
    "\n",
    "def text_processing(full_text):\n",
    "    findings_pattern = r\"FINDINGS:(.*?)\"\n",
    "    findings_match = re.search(findings_pattern, full_text, re.DOTALL)\n",
    "    impression_pattern = r\"IMPRESSION:(.*?)\"\n",
    "    impression_match = re.search(impression_pattern, full_text, re.DOTALL)\n",
    "    if findings_match and impression_match:\n",
    "        findings_start = findings_match.span()[0]\n",
    "        impression_start = impression_match.span()[0]\n",
    "        if findings_start <= impression_start :\n",
    "            text = full_text[findings_start:]\n",
    "        else:\n",
    "            text = full_text[impression_start:]\n",
    "    elif findings_match and (not impression_match):\n",
    "        findings_start = findings_match.span()[0]\n",
    "        text = full_text[findings_start:]\n",
    "    elif (not findings_match) and impression_match:\n",
    "        impression_start = impression_match.span()[0]\n",
    "        text = full_text[impression_start:]\n",
    "    else:\n",
    "        text = full_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0705553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/mimic3_cxr_jpg/mimic-cxr-dataset.csv')\n",
    "df = standardize_view_position_direct(df)\n",
    "df = df[(df['ViewPosition'] == \"PA\") | (df['ViewPosition'] == \"AP\")].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 11.75it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_rows = []\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    note = load_text('/data/mimic3_cxr_jpg/'+row['path'])\n",
    "    note = text_processing(note)\n",
    "    embedding = text2embedding(client, model, note)\n",
    "    embedding_rows.append(embedding)\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760086a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218132"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['study_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5770b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243335"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7678b3",
   "metadata": {},
   "source": [
    "# DataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5147e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_path_refine(row):\n",
    "    return f'/data/mimic3_cxr_jpg/mimic-cxr-jpg-2.0.0.physionet.org/files/p{str(row['subject_id'])[:2]}/p{row['subject_id']}/s{row['study_id']}/{row['dicom_id']}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612f517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimic_train_df = pd.read_csv('/data/mimic3_cxr_jpg/train_with_view_embeddings.csv')\n",
    "# mimic_train_df['ImagePath'] = mimic_train_df.apply(image_path_refine, axis=1)\n",
    "# rex_train_df = pd.read_csv('/data/ReXGradient-160K/metadata/train_with_view_embeddings.csv')\n",
    "# rex_train_df['ImagePath'] = rex_train_df['ImagePath'].apply(lambda x : x.replace('../', '/data/ReXGradient-160K/'))\n",
    "# train_df = pd.concat([mimic_train_df[['ImagePath', 'embeddings']],rex_train_df[['ImagePath', 'embeddings']]], axis=0).reset_index(drop=True)\n",
    "train_df = pd.read_csv('/data/code/CXR_embedding_research/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abca79a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_tool = KorniaGPUAugmentation()\n",
    "dataloader = create_dataloader(train_df, label_type='embedding', batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c47e3f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea38c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at /data/models/vit-base-patch16-384 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, ViTFeatureExtractor \n",
    "model = ViTModel.from_pretrained('/data/models/vit-base-patch16-384')\n",
    "model = custom_vit_embed(model)\n",
    "model = model.to('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas=(0.9,0.999), eps=1e-6, weight_decay=0.01, amsgrad=False)\n",
    "criterion = torch.nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c553962",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(dataloader))\n",
    "pixel_values = augment_tool(imgs, True)\n",
    "pixel_values = pixel_values.to('cuda')\n",
    "# pixel_values = imgs.to('cuda')\n",
    "labels = labels.to('cuda')\n",
    "output = model(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fd466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(32).to('cuda')\n",
    "loss = criterion(output, labels, ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa01ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4728fba1",
   "metadata": {},
   "source": [
    "# Text augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e028e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_train_df = pd.read_csv('/data/mimic3_cxr_jpg/train_with_view_embeddings.csv')\n",
    "mimic_train_df['ImagePath'] = mimic_train_df.apply(image_path_refine, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc253d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de91823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820dcb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
